model:
  d_fft: 1024
  d_model: 512
  dropout: 0.1
  heads: 4
  layers: 6
  type: transformer
paths:
  checkpoint_dir: checkpoints
  data_dir: datasets
preprocessing:
  char_repeats: 3
  languages:
  - en_us
  lowercase: true
  n_val: 5000
  phoneme_symbols:
  - a
  - b
  - d
  - e
  - f
  - g
  - h
  - i
  - j
  - k
  - l
  - m
  - n
  - o
  - p
  - r
  - s
  - t
  - u
  - v
  - w
  - x
  - y
  - z
  - "\xE6"
  - "\xE7"
  - "\xF0"
  - "\xF8"
  - "\u014B"
  - "\u0153"
  - "\u0250"
  - "\u0251"
  - "\u0254"
  - "\u0259"
  - "\u025B"
  - "\u025D"
  - "\u0279"
  - "\u0261"
  - "\u026A"
  - "\u0281"
  - "\u0283"
  - "\u028A"
  - "\u028C"
  - "\u028F"
  - "\u0292"
  - "\u0294"
  - "\u02C8"
  - "\u02CC"
  - "\u02D0"
  - "\u0303"
  - "\u030D"
  - "\u0325"
  - "\u0329"
  - "\u032F"
  - "\u0361"
  - "\u03B8"
  - ''''
  text_symbols: abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ
training:
  batch_size: 48
  batch_size_val: 48
  checkpoint_steps: 10000
  ddp_backend: nccl
  ddp_host: localhost
  ddp_post: '12355'
  epochs: 15
  generate_steps: 500
  learning_rate: 0.0001
  n_generate_samples: 10
  scheduler_plateau_factor: 0.5
  scheduler_plateau_patience: 10
  store_phoneme_dict_in_model: true
  validate_steps: 500
  warmup_steps: 100
